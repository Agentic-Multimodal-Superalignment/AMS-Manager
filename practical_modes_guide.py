#!/usr/bin/env python3
"""
ğŸ§™â€â™‚ï¸ PRACTICAL OPEN INTERPRETER MODES GUIDE

Ready-to-use commands for your Ollama setup!
"""

print("ğŸ¯ PRACTICAL OPEN INTERPRETER COMMANDS")
print("=" * 50)
print("ğŸ§™â€â™‚ï¸ These commands will work with your Ollama setup!")
print("=" * 50)

print("\nğŸ  LOCAL MODE (Recommended Starting Point)")
print("   Perfect for privacy + your Ollama models")
print("")
local_commands = [
    "# Basic local mode",
    "python -c \"from interpreter import interpreter; interpreter.offline=True; interpreter.auto_run=False; interpreter.llm.api_base='http://localhost:11434/v1'; interpreter.llm.model='qwen2.5-coder'; interpreter.chat()\"",
    "",
    "# Or use our Merlin integration",
    "python src/ams_manager/main.py chat"
]

for cmd in local_commands:
    print(f"   {cmd}")

print("\nğŸ›¡ï¸ SAFE LOCAL MODE (Best for Learning)")
print("   Maximum safety + local privacy")
print("")
safe_commands = [
    "# Safe mode with confirmations",
    "python -c \"from interpreter import interpreter; interpreter.offline=True; interpreter.auto_run=False; interpreter.llm.api_base='http://localhost:11434/v1'; interpreter.llm.model='qwen2.5-coder'; print('Safe local mode ready! Type your questions.'); interpreter.chat()\"",
]

for cmd in safe_commands:
    print(f"   {cmd}")

print("\nğŸ‘ï¸ VISION MODE (If Your Model Supports It)")
print("   Analyze images and screenshots")
print("")
vision_commands = [
    "# Vision + local mode",
    "python -c \"from interpreter import interpreter; interpreter.offline=True; interpreter.auto_run=False; interpreter.llm_supports_vision=True; interpreter.llm.api_base='http://localhost:11434/v1'; interpreter.llm.model='llava'; interpreter.chat()\"",
    "",
    "# Note: You'll need a vision model like llava"
]

for cmd in vision_commands:
    print(f"   {cmd}")

print("\nâš¡ FAST MODE (Quick Tasks)")
print("   Concise responses for simple questions")
print("")
fast_commands = [
    "# Fast local mode", 
    "python -c \"from interpreter import interpreter; interpreter.offline=True; interpreter.auto_run=False; interpreter.llm.api_base='http://localhost:11434/v1'; interpreter.llm.model='qwen2.5-coder'; interpreter.llm.max_tokens=150; print('Fast mode ready!'); interpreter.chat()\"",
]

for cmd in fast_commands:
    print(f"   {cmd}")

print("\nğŸ”„ LOOP MODE (Complex Projects)")
print("   âš ï¸  Use carefully - monitors required!")
print("")
loop_commands = [
    "# Loop mode with safety",
    "python -c \"from interpreter import interpreter; interpreter.offline=True; interpreter.auto_run=False; interpreter.llm.api_base='http://localhost:11434/v1'; interpreter.llm.model='qwen2.5-coder'; print('Loop mode ready - be specific about goals!'); interpreter.chat()\"",
    "",
    "# Then ask: 'Create a complete Python project with tests'"
]

for cmd in loop_commands:
    print(f"   {cmd}")

print("\nğŸ­ INTERACTIVE TESTING SCRIPT")
print("   Let's create a simple test you can run right now!")

# Create a simple test script
test_script = '''
print("ğŸ§™â€â™‚ï¸ Testing Open Interpreter Modes...")

try:
    from interpreter import interpreter
    print("âœ… Open Interpreter available!")
    
    # Test configuration
    interpreter.offline = True
    interpreter.auto_run = False
    interpreter.llm.api_base = "http://localhost:11434/v1"
    interpreter.llm.api_key = "fake_key"
    interpreter.llm.model = "qwen2.5-coder"
    
    print("ğŸ”§ Configuration successful!")
    print("ğŸ¯ You can now use these modes:")
    print("   â€¢ Local mode âœ…")
    print("   â€¢ Safe mode âœ…") 
    print("   â€¢ Vision mode (if llava model available)")
    print("   â€¢ Fast mode âœ…")
    print("   â€¢ Loop mode âœ…")
    
    print("\\nğŸš€ Ready to start! Try asking:")
    print("   'Write a Python function to calculate fibonacci'")
    print("   'Help me organize my project files'")
    print("   'Explain how local AI models work'")
    
    # Optional: Start chat
    start_chat = input("\\nğŸ¯ Start interactive chat now? (y/n): ")
    if start_chat.lower() == 'y':
        print("ğŸ§™â€â™‚ï¸ Starting chat mode...")
        interpreter.chat()
    else:
        print("âœ… Setup complete! Use the commands above to start.")
        
except ImportError:
    print("âŒ Open Interpreter not found")
    print("ğŸ’¡ Install with: uv pip install open-interpreter")
except Exception as e:
    print(f"âŒ Error: {e}")
'''

print("\nğŸ“ INSTANT TEST SCRIPT:")
print("   Save this as 'test_modes.py' and run it:")
print("-" * 50)
print(test_script)
print("-" * 50)

print("\nğŸ¯ QUICK START RECOMMENDATIONS:")
recommendations = [
    ("ğŸ¥‡ First Time", "Use local + safe mode", "Learn without risk"),
    ("ğŸš€ Daily Use", "Local mode", "Fast, private, efficient"),
    ("ğŸ¨ Creative Work", "Vision mode", "Analyze images and designs"),
    ("âš¡ Quick Questions", "Fast mode", "Short, concise answers"),
    ("ğŸ—ï¸ Big Projects", "Loop mode", "Persistent task completion"),
    ("ğŸ›¡ï¸ Sensitive Data", "Local + safe", "Maximum privacy + protection")
]

for rank, mode, desc in recommendations:
    print(f"   {rank} {mode}: {desc}")

print("\nğŸ§™â€â™‚ï¸ MERLIN'S FINAL WISDOM:")
print("=" * 50)
final_tips = [
    "ğŸ¯ Start with auto_run=False always",
    "ğŸ  Local mode = privacy + speed",
    "ğŸ›¡ï¸ Safe mode = protection + learning",
    "ğŸ‘ï¸ Vision mode = image understanding",
    "ğŸ”„ Loop mode = complex automation",
    "âš¡ Fast mode = quick answers",
    "ğŸ–¥ï¸ OS mode = computer control (advanced!)"
]

for tip in final_tips:
    print(f"   {tip}")

print("\nâœ¨ You now have the power of ALL Open Interpreter modes!")
print("ğŸš€ Go forth and create something magical!")
print("ğŸ§™â€â™‚ï¸ Remember: With great power comes great responsibility!")
